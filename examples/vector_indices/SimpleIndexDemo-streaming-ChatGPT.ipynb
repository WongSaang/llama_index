{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3a8ef8-f387-4b90-9b1c-97d8ca707a02",
   "metadata": {},
   "source": [
    "### Integrating Streaming into LlamaIndex via Callback Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02cc0b-d9f6-4efd-bc6a-3f8fc534d85b",
   "metadata": {},
   "source": [
    "This demo shows how you can integrate streaming capabilities into a LlamaIndex app through LangChain's callback manager.\n",
    "\n",
    "LangChain's callback manager provides an `on_llm_new_token` method that's called for each token returned in a streaming setting.\n",
    "\n",
    "The challenge with integrating with LlamaIndex is that we only want `on_llm_new_token` to be called for the *final* response,\n",
    "rather than for every intermediate response. This notebook below shows you how to set that up.\n",
    "\n",
    "Key components:\n",
    "- `ChatOpenAI` LLM class with `streaming=True`\n",
    "- `ChatGPTLLMPredictor` with LLM and `CallbackManager` specified\n",
    "- call `index.query` with `streaming=True`\n",
    "\n",
    "A few notes:\n",
    "- We've provided a demo `StreamStdoutCallbackHandler` that outputs tokens to stdout, but would highly encourage you to develop your own CallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ec7f3f-5a90-494c-b035-336063453210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.CRITICAL)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Uncomment if you want to temporarily disable logger\n",
    "# logger = logging.getLogger()\n",
    "# logger.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae41886b-ffe7-40f4-a76c-676872b1aa8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import CallbackManager\n",
    "from gpt_index.llm_predictor.chatgpt import ChatGPTLLMPredictor\n",
    "from gpt_index.langchain_helpers.callbacks import StreamStdoutCallbackHandler\n",
    "\n",
    "from gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader, ServiceContext, LLMPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b82d496-3969-47e9-b444-8554533caa5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11a53cc3-44ec-41ca-a0b7-3224682ef8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: please define your own callback handler/manager\n",
    "# the StreamCallbackHandler is primarily used for demo purposes\n",
    "callback_manager = CallbackManager(handlers=[StreamStdoutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe4d81d-353a-4281-b0c3-7c898b5bb58d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define LLM + LLMPredictor\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\n",
    "llm_predictor = ChatGPTLLMPredictor(llm=llm, callback_manager=callback_manager)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa28f579-913f-4014-b4e1-1f352bdf365e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2cf5b35-93e3-4ff2-85f3-d0b9e431c756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing and programming outside of school growing up. They wrote short stories and tried programming on an IBM 1401 in 9th grade, using an early version of Fortran. With microcomputers, they started programming more and wrote simple games, a rocket prediction program, and a word processor. Despite enjoying programming, they initially planned to study philosophy in college."
     ]
    }
   ],
   "source": [
    "response = index.query(\n",
    "    \"What did the author do growing up?\", \n",
    "    service_context=service_context,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dffb6-a393-437a-814d-7fc87ed5de9e",
   "metadata": {},
   "source": [
    "### Note: What if you just use the callback manager as part of the LLM class directly?\n",
    "\n",
    "This means that intermediate LLM calls will also be logged, which may or may not be what you'd want.\n",
    "\n",
    "We show an example where we just attach the callback manager to the LLM class, and make 0 modifications on the LlamaIndex side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "439353be-3680-47c0-aff1-cb1b8b2bd9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define \n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True, callback_manager=callback_manager)\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903ba919-6ced-4d46-86d9-468064fc56ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing and programming outside of school when growing up. They wrote short stories and started programming on an IBM 1401 using an early version of Fortran. They later got a TRS-80 microcomputer and started programming simple games, a rocket prediction program, and a word processor.The author worked on writing and programming outside of school when growing up. They wrote short stories and started programming on an IBM 1401 using an early version of Fortran. They later got a TRS-80 microcomputer and started programming simple games, a rocket prediction program, and a word processor. While in a PhD program in computer science at Harvard, the author took art classes and decided to become an artist. They applied to RISD and the Accademia di Belli Arti in Florence and ended up studying at RISD. After finishing the BFA program at RISD, the author received an invitation to take the entrance exam at the Accademia, and despite being a foreigner and having to write an essay in Italian, managed to pass the exam.The context provided is not useful for refining the original answer, which correctly states that the author worked on writing and programming outside of school when growing up and later became an artist after studying at RISD."
     ]
    }
   ],
   "source": [
    "response = index.query(\n",
    "    \"What did the author do growing up?\", \n",
    "    service_context=service_context,\n",
    "    similarity_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40f09b-d19a-4e6b-a615-78e356e190ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
